import streamlit as st
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings # ë¬´ë£Œ ì„ë² ë”©
from langchain.chains import RetrievalQA

# .env ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="RAG Chatbot Study", page_icon="ğŸ“š")
st.title("ğŸ“š 0ì› RAG ì±—ë´‡ (OpenRouter)")

# 1. ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì¸í•´ ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask") # í•œêµ­ì–´ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸

embeddings = load_embeddings()

# 2. íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf', 'docx', 'txt'])

if uploaded_file:
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    if uploaded_file.name.endswith(".pdf"):
        loader = PyPDFLoader(uploaded_file.name)
    elif uploaded_file.name.endswith(".docx"):
        loader = Docx2txtLoader(uploaded_file.name)
    else:
        loader = TextLoader(uploaded_file.name)
    
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)

    # 3. ë²¡í„° ì €ì¥ì†Œ ìƒì„± (Chroma)
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    st.success("ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ!")

    # 4. ì±„íŒ… ë£¨í”„
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 5. OpenRouter ë¬´ë£Œ ëª¨ë¸ ì—°ê²°
        llm = ChatOpenAI(
            model_name="meta-llama/llama-3.3-70b-instruct:free", # OpenRouter ë¬´ë£Œ ëª¨ë¸
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            base_url="https://openrouter.ai/api/v1",
        )

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever()
        )
        
        with st.chat_message("assistant"):
            response = qa_chain.invoke(prompt)
            answer = response['result']
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})

    os.remove(uploaded_file.name)