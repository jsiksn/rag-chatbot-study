import streamlit as st
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers.multi_query import MultiQueryRetriever # ë©€í‹° ì¿¼ë¦¬ ì¶”ê°€
from langchain.prompts import PromptTemplate # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€

# .env ë¡œë“œ
load_dotenv()

st.set_page_config(
  page_title="Hybrid RAG Chatbot", 
  page_icon="ğŸ’¬",
  layout="wide", 
  initial_sidebar_state="expanded" 
)
st.title("ğŸ’¬ Hybrid RAG Chatbot")

# 1. ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸ ì„¤ì •
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

embeddings = load_embeddings()

# 2. íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf', 'docx', 'txt'])

if uploaded_file:
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    if uploaded_file.name.endswith(".pdf"):
        loader = PyPDFLoader(uploaded_file.name)
    elif uploaded_file.name.endswith(".docx"):
        loader = Docx2txtLoader(uploaded_file.name)
    else:
        loader = TextLoader(uploaded_file.name)
    
    docs = loader.load()
    # ì¸ë¬¼ ì •ë³´ë¥¼ ìœ„í•´ ì²­í¬ ì‚¬ì´ì¦ˆë¥¼ ì•½ê°„ í‚¤ìš°ê³  ì˜¤ë²„ë©ì„ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # 3. í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„±
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    bm25_retriever = BM25Retriever.from_documents(splits)
    bm25_retriever.k = 3

    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5]
    )

    st.success("ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ! ì´ì œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")

    # ---------------------------------------------------------
    # 1. LLM ì„¤ì •ì„ ì±„íŒ… ë£¨í”„ ë°–ìœ¼ë¡œ ì´ë™ (ë¶„ì„ ë²„íŠ¼ì—ì„œë„ ì¨ì•¼ í•˜ë‹ˆê¹Œìš”)
    # ---------------------------------------------------------
    llm = ChatOpenAI(
        model_name="meta-llama/llama-3.3-70b-instruct:free",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0
    )

    # ---------------------------------------------------------
    # 2. ì‚¬ì´ë“œë°” ë° Expander(ì ‘ì´ì‹ ë©”ë‰´) ì ìš©
    # ---------------------------------------------------------
    if "rel_map" not in st.session_state:
        st.session_state.rel_map = None

    with st.sidebar:
        st.header("ğŸ—ºï¸ ê·¸ë˜í”„ RAG ë§›ë³´ê¸°")
        st.write("ë¬¸ì„œì˜ ì¸ë¬¼ ê´€ê³„ë¥¼ í•œëˆˆì— íŒŒì•…í•˜ì„¸ìš”.")
        
        if st.button("ğŸ“Š ì¸ë¬¼ ê´€ê³„ë„ ë¶„ì„ ì‹œì‘"):
            with st.spinner("ê´€ê³„ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                # í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ë¡œ í•µì‹¬ ë§¥ë½ ì¶”ì¶œ
                rel_docs = ensemble_retriever.invoke("ì¸ë¬¼ë“¤ ì‚¬ì´ì˜ ê´€ê³„ì™€ ì£¼ìš” ì‚¬ê±´")
                rel_context = "\n".join([d.page_content for d in rel_docs])
                
                rel_prompt = f"""
                ì•„ë˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¸ë¬¼ ë° ì¡°ì§ ê°„ì˜ ê´€ê³„ë¥¼ [í‘œ]ë¡œ ìš”ì•½í•´ì¤˜.
                í˜•ì‹: [ëŒ€ìƒ A | ê´€ê³„ | ëŒ€ìƒ B | ìƒì„¸ ì„¤ëª…]
                - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•  ê²ƒ.
                - ì ˆëŒ€ë¡œ í•œì(æ¼¢å­—)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.
                
                ë‚´ìš©:
                {rel_context}
                """
                rel_response = llm.invoke(rel_prompt)
                st.session_state.rel_map = rel_response.content
        
        # âœ¨ 2ë²ˆ ì˜µì…˜: ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì ‘ì´ì‹ ë©”ë‰´ë¡œ í‘œì‹œ
        if st.session_state.rel_map:
            st.divider()
            # expanded=Trueë¡œ ì„¤ì •í•˜ë©´ ë¶„ì„ ì§í›„ì— ìë™ìœ¼ë¡œ í¼ì³ì§‘ë‹ˆë‹¤.
            with st.expander("ğŸ“ ì¸ë¬¼ ê´€ê³„ë„ ìƒì„¸ë³´ê¸°", expanded=True):
                st.markdown(st.session_state.rel_map)

    # ---------------------------------------------------------
    # 3. ì±„íŒ… ë£¨í”„ (ê¸°ì¡´ ì½”ë“œì—ì„œ LLM ì„¤ì • ë¶€ë¶„ë§Œ ì œì™¸í•˜ë©´ ë©ë‹ˆë‹¤)
    # ---------------------------------------------------------
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # (LLM ì„¤ì • ë¶€ë¶„ì€ ìœ„ë¡œ ì˜®ê²¼ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ)
        
        # âœ¨ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
        template = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì—„ìˆ˜í•˜ì—¬ [Context]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì§€ì¹¨]
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. **ì ˆëŒ€ë¡œ í•œì(æ¼¢å­—)ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”.** ëª¨ë“  ë‹¨ì–´ëŠ” í•œê¸€ë¡œë§Œ í‘œê¸°í•˜ì„¸ìš”.
3. [Context]ì˜ ë‚´ìš©ì„ ìµœëŒ€í•œ í™œìš©í•˜ë˜, ì§ì ‘ì ì¸ ë‹µì´ ì—†ë‹¤ë©´ ê´€ë ¨ ë‹¨ì„œë¼ë„ ì°¾ì•„ ì„¤ëª…í•˜ì„¸ìš”.
4. ì •ë§ ê´€ë ¨ ë‚´ìš©ì´ ì—†ë‹¤ë©´ "ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

[Context]: {context}

ì§ˆë¬¸: {question}
ë‹µë³€:"""

        prompt_template = PromptTemplate(
            template=template, 
            input_variables=["context", "question"]
        )

        # 6. ë©€í‹° ì¿¼ë¦¬ ë¦¬íŠ¸ë¦¬ë²„
        advanced_retriever = MultiQueryRetriever.from_llm(
            retriever=ensemble_retriever, 
            llm=llm
        )

        # 7. QA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=advanced_retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt_template}
        )
        
        with st.chat_message("assistant"):
            with st.spinner("ë¬¸ì„œë¥¼ ê¼¼ê¼¼íˆ ì½ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = qa_chain.invoke(prompt)
                answer = response['result']
                source_documents = response.get('source_documents', [])

            st.markdown(answer)
            # ... (ì°¸ê³  ë¬¸í—Œ ì¶œë ¥ ë¡œì§ ë™ì¼) ...

            st.session_state.messages.append({"role": "assistant", "content": answer})

    # íŒŒì¼ ì‚­ì œ ë¡œì§ ìœ ì§€
    try:
        os.remove(uploaded_file.name)
    except:
        pass